# Made by Cyto
#     　　　　 ＿ ＿
# 　　　　　／＞　　 フ
# 　　　　　|   _　 _l
# 　 　　　／` ミ＿xノ
# 　　 　 /　　　 　 |
# 　　　 /　 ヽ　　 ﾉ
# 　 　 │　　|　|　|
# 　／￣|　　 |　|　|
# 　| (￣ヽ＿_ヽ_)__)
# 　＼二つ ；
import os
import os.path as osp
import json
from typing import List

import cv2
from PIL import Image
import numpy as np
from torch.utils.data import Dataset


class Algonauts2023Raw(Dataset):
    """
        Load original data for Algonauts2023 dataset
    """

    def __init__(self,
                 data_path: str,
                 caption_file: str = None,
                 hemisphere: str = "L",
                 transform=None,
                 train: bool = True,
                 return_img_ids: bool = False,
                 return_pil: bool = False):
        """
            Initialize a torch.utils.data.Dataset object for algonauts2023 dataset

            Args:
                data_path,              str, path to the algonauts2023 dataset which contains only ONE subject
                caption_file,           str, if provided, __getitem__() will return with a caption of the image
                hemisphere,             str, select which hemisphere of the brain to be modeled
                                            can ONLY select "L" or "R"
                                            and ONLY applicable when train is TRUE
                transform,              torchvision.transform methods, apply normalization to the dataset
                train,                  bool, training data will be loaded if True. Test data otherwise.
                return_img_ids,         bool, return image ids, only used for feature extraction
                return_pil,             bool, return PIL.Image format instead of numpy.ndarray for image
        """

        # collect data paths
        path_struct = osp.join(data_path, "{}_split")
        self.dataset = list()
        self.transform = transform
        self.train = train
        self.return_img_ids = return_img_ids
        self.return_pil = return_pil

        if caption_file:
            with open(caption_file, "r", encoding="UTF-8") as f:
                self.captions = json.load(f)

        if train:
            shared_path = osp.join(
                path_struct.format("training"), "training_{}")
            if hemisphere == "L":
                self.fmri = np.load(osp.join(shared_path.format(
                    "fmri"), "lh_training_fmri.npy"))
            elif hemisphere == "R":
                self.fmri = np.load(osp.join(shared_path.format(
                    "fmri"), "rh_training_fmri.npy"))

            self.feature_path = shared_path.format("images")

        else:
            self.feature_path = osp.join(
                path_struct.format("test"), "test_images")

        self.dataset = list(os.listdir(self.feature_path))

        # sorted in ascending order if not train set
        if not train:
            self.dataset = sorted(self.dataset, key=lambda x: int(
                x.split("_")[0].split("-")[-1]) - 1)

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index: int):
        """
            Load designated sample

            Arg:
                index,          int, sample id

            Returns:
                image,          np.ndarray or PIL.Image, the image
                fmri,           np.ndarray, the hemisphere FMRI data generated by the image
                img_ids,        str, image ids, only used for feature extraction
                captions,       str, coco captions of the image, only used when captions_file is provided when initialize the dataset
        """

        feat_file = self.dataset[index]
        feat_idx = int(feat_file.split("_")[0].split("-")[-1]) - 1

        if self.captions:
            tmp = feat_file[:-4]
            cap = self.captions[str(int(tmp.split("_")[1].split("-")[-1]))]
            # randomly pick one captions
            cap = np.random.choice(cap)

        if self.return_pil:
            img = Image.open(osp.join(self.feature_path, feat_file))

            if img.mode != "RGB":
                img = img.convert(mode="RGB")

        else:
            img = cv2.imread(osp.join(self.feature_path, feat_file)
                             ).astype(np.float32)

            # convert BGR to RGB
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        if self.transform:
            img = self.transform(img)

        if self.return_img_ids:
            if self.captions:
                return img, self.fmri[feat_idx] if self.train else 0, cap, feat_file
            return img, self.fmri[feat_idx] if self.train else 0, feat_file
        else:
            if self.captions:
                return img, self.fmri[feat_idx] if self.train else 0, cap
            return img, self.fmri[feat_idx] if self.train else 0, cap


class Algonauts2023Feature(Dataset):

    """
        Load processed features of Algonauts2023 dataset
    """

    def __init__(self, data_path: str, extractor: str, layer: str,  hemisphere: str = "L", train: bool = True):
        """
            Initialize a torch.utils.data.Dataset object for algonauts2023 dataset

            Args:
                data_path,              str, path to the algonauts2023 dataset which contains only ONE subject
                extractor,              str, name of extractor of the feature
                layer,                  str, name of layer of extractor of the feauture
                hemisphere,             str, select which hemisphere of the brain to be modeled
                                            can ONLY select "L" or "R"
                                            and ONLY applicable when train is TRUE
                train,                  bool, training data will be loaded if True. Test data otherwise.
        """

        # collect data paths
        path_struct = osp.join(data_path, "{}_split")
        self.dataset = list()
        self.train = train

        if train:
            shared_path = osp.join(
                path_struct.format("training"), "training_{}")
            fmri_path = shared_path.format("fmri")
            if hemisphere == "L":
                self.fmri = np.load(
                    osp.join(fmri_path, "lh_training_fmri.npy"))
            elif hemisphere == "R":
                self.fmri = np.load(
                    osp.join(fmri_path, "rh_training_fmri.npy"))

            self.feature_path = shared_path.format("features")

        else:
            self.feature_path = osp.join(
                path_struct.format("test"), "test_features")

        self.feature_path = osp.join(self.feature_path, extractor, layer)

        self.dataset = list(os.listdir(self.feature_path))

        # sorted in ascending order if not train set
        if not train:
            self.dataset = sorted(self.dataset, key=lambda x: int(
                x.split("_")[0].split("-")[-1]) - 1)

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index: int):
        """
            Load designated sample

            Arg:
                index,          int, sample id

            Returns:
                feature,        np.ndarray, the 1d numpy array of image features
                fmri,           np.ndarray, the hemisphere FMRI data generated by the image
        """

        feat_file = self.dataset[index]
        feat_idx = int(feat_file.split("_")[0].split("-")[-1]) - 1

        # load
        feature = np.load(osp.join(self.feature_path, feat_file)
                          ).astype(np.float32)

        return feature, self.fmri[feat_idx] if self.train else 0


def __get_img_features(path: str):

    all_files = os.listdir(path)

    # sort by file ID
    all_files = sorted(all_files, key=lambda x: int(
        x.split("_")[0].split("-")[-1]) - 1)

    features = list()
    for file in all_files:

        features.append(np.load(osp.join(path, file)))

    return np.vstack(features)


def get_features(data_paths: List[str]):

    features = list()
    for path in data_paths:
        features.append(__get_img_features(path))

    return np.hstack(features)


if __name__ == "__main__":

    lst = [
        "../../data.nosync/subj01/training_split/training_features/vit-gpt2-image-captioning/encoder-last1-pca-512",
        "../../data.nosync/subj01/training_split/training_features/resnet50-imagenet1k-v2/avgpool"]

    tst = get_features(lst)

    print(tst.shape)
