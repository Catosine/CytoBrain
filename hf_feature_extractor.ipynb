{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    \"./backbone.nosync/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\n",
    "    \"./backbone.nosync/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./backbone.nosync/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams, \"return_dict_in_generate\": True, \"output_hidden_states\": True}\n",
    "\n",
    "# ['a woman in a hospital bed with a woman in a hospital bed']\n",
    "# predict_step(['doctor.e16ba4e4.jpg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step(image_paths):\n",
    "  images = []\n",
    "  for image_path in image_paths:\n",
    "    i_image = Image.open(image_path)\n",
    "    if i_image.mode != \"RGB\":\n",
    "      i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    images.append(i_image)\n",
    "\n",
    "  pixel_values = feature_extractor(\n",
    "      images=images, return_tensors=\"pt\")\n",
    "\n",
    "  pixel_values = pixel_values.pixel_values\n",
    "  \n",
    "  pixel_values = pixel_values.to(device)\n",
    "  \n",
    "  #outputs = model(pixel_values, decoder_input_embeds=pixel_values)\n",
    "  outputs = model.generate(pixel_values, **gen_kwargs)\n",
    "  print(len(outputs.encoder_hidden_states))\n",
    "  output_ids = outputs.sequences\n",
    "\n",
    "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  return preds\n",
    "\n",
    "\n",
    "predict_step(['../../data.nosync/subj01/test_split/test_images/test-0011_nsd-04735.png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Algonauts2023Raw(Dataset):\n",
    "    \"\"\"\n",
    "        Load original data for Algonauts2023 dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, hemisphere: str = \"L\", transform=None, train: bool = True, return_img_ids: bool = False):\n",
    "        \"\"\"\n",
    "            Initialize a torch.utils.data.Dataset object for algonauts2023 dataset\n",
    "\n",
    "            Args:\n",
    "                data_path,              str, path to the algonauts2023 dataset which contains only ONE subject\n",
    "                hemisphere,             str, select which hemisphere of the brain to be modeled\n",
    "                                            can ONLY select \"L\" or \"R\"\n",
    "                                            and ONLY applicable when train is TRUE\n",
    "                transform,              torchvision.transform methods, apply normalization to the dataset\n",
    "                train,                  bool, training data will be loaded if True. Test data otherwise.\n",
    "                return_img_ids,         bool, return image ids, only used for feature extraction\n",
    "        \"\"\"\n",
    "\n",
    "        # collect data paths\n",
    "        path_struct = osp.join(data_path, \"{}_split\")\n",
    "        self.dataset = list()\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.return_img_ids = return_img_ids\n",
    "\n",
    "        if train:\n",
    "            shared_path = osp.join(\n",
    "                path_struct.format(\"training\"), \"training_{}\")\n",
    "            if hemisphere == \"L\":\n",
    "                self.fmri = np.load(osp.join(shared_path.format(\n",
    "                    \"fmri\"), \"lh_training_fmri.npy\"))\n",
    "            elif hemisphere == \"R\":\n",
    "                self.fmri = np.load(osp.join(shared_path.format(\n",
    "                    \"fmri\"), \"rh_training_fmri.npy\"))\n",
    "\n",
    "            self.feature_path = shared_path.format(\"images\")\n",
    "\n",
    "        else:\n",
    "            self.feature_path = osp.join(\n",
    "                path_struct.format(\"test\"), \"test_images\")\n",
    "\n",
    "        self.dataset = list(os.listdir(self.feature_path))\n",
    "\n",
    "        # sorted in ascending order if not train set\n",
    "        if not train:\n",
    "            self.dataset = sorted(self.dataset, key=lambda x: int(\n",
    "                re.findall(\"\\d{4}\", x)[0]) - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "            Load designated sample\n",
    "\n",
    "            Arg:\n",
    "                index,          int, sample id\n",
    "\n",
    "            Returns:\n",
    "                image,          np.ndarray, the 3d numpy array of the image used to retrive fmri data\n",
    "                fmri,           np.ndarray, the hemisphere FMRI data generated by the image\n",
    "                img_ids,        str, image ids, only used for feature extraction\n",
    "        \"\"\"\n",
    "\n",
    "        feat_file = self.dataset[index]\n",
    "        feat_idx = int(re.findall(\"\\d{4}\", feat_file)[0]) - 1\n",
    "\n",
    "        img = Image.open(osp.join(self.feature_path, feat_file))\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(mode=\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.return_img_ids:\n",
    "            return img, self.fmri[feat_idx] if self.train else 0, feat_file\n",
    "        else:\n",
    "            return img, self.fmri[feat_idx] if self.train else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = \"subj01\"\n",
    "\n",
    "path = \"../../data.nosync/{}\".format(subj)\n",
    "save = \"../../data.nosync/{}/training_split/training_features/vit-gpt2-image-captioning/decoder-raw\".format(\n",
    "    subj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Algonauts2023Raw(path, return_img_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def func(x):\n",
    "\n",
    "    imgs = list()\n",
    "    ids = list()\n",
    "    for img, _, id in x:\n",
    "        imgs.append(img)\n",
    "        ids.append(id)\n",
    "\n",
    "    return imgs, ids\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "ids = list()\n",
    "features = list()\n",
    "for img, id in tqdm(DataLoader(dset, batch_size=64, num_workers=12, collate_fn=func)):\n",
    "\n",
    "    pixel_values = feature_extractor(\n",
    "        images=img, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    feats = model.generate(pixel_values, **gen_kwargs)\n",
    "    feats = [x.cpu() for x in feats.encoder_hidden_states]\n",
    "\n",
    "    features.append(feats)\n",
    "    ids.append(id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9841/9841 [00:07<00:00, 1324.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "avg_save = \"../../data.nosync/{}/training_split/training_features/vit-gpt2-image-captioning/decoder-cls\".format(\n",
    "    subj)\n",
    "\n",
    "if not os.path.isdir(avg_save):\n",
    "\n",
    "    os.makedirs(avg_save)\n",
    "\n",
    "for x in tqdm(os.listdir(save)):\n",
    "    dat = np.load(os.path.join(save, x))\n",
    "    dat = dat[0, :].reshape(-1)\n",
    "    \n",
    "    #dat = np.mean(dat, axis=0).reshape(-1)\n",
    "    np.save(os.path.join(avg_save, x), dat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "feats = list()\n",
    "files = os.listdir(save)\n",
    "for x in files:\n",
    "    dat = np.load(os.path.join(save, x))\n",
    "    feats.append(np.load(os.path.join(save, x)).reshape(-1))\n",
    "\n",
    "feats = np.vstack(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=64)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=64)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=64)\n",
    "\n",
    "pca.fit(X=feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_feats = pca.transform(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9841, 64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_save = \"../../data.nosync/{}/training_split/training_features/vit-gpt2-image-captioning/decoder-pca-64\".format(\n",
    "    subj)\n",
    "\n",
    "if not os.path.isdir(pca_save):\n",
    "\n",
    "    os.makedirs(pca_save)\n",
    "\n",
    "for f, x in zip(reduced_feats, files):\n",
    "    np.save(os.path.join(pca_save, x), f.astype(np.float32))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algonauts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
