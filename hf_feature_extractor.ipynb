{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 4.50k/4.50k [00:00<00:00, 2.20MB/s]\n",
      "Downloading: 100%|██████████| 937M/937M [00:33<00:00, 29.4MB/s] \n",
      "Downloading: 100%|██████████| 228/228 [00:00<00:00, 112kB/s]\n",
      "Downloading: 100%|██████████| 241/241 [00:00<00:00, 124kB/s]\n",
      "Downloading: 100%|██████████| 779k/779k [00:00<00:00, 1.07MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 595kB/s] \n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 1.40MB/s]\n",
      "Downloading: 100%|██████████| 120/120 [00:00<00:00, 60.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor as ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    \"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\n",
    "    \"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\n",
    "    \"max_length\": max_length, \n",
    "    \"num_beams\": num_beams, \n",
    "    \"return_dict_in_generate\": True, \n",
    "    \"output_hidden_states\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Algonauts2023Raw(Dataset):\n",
    "    \"\"\"\n",
    "        Load original data for Algonauts2023 dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, hemisphere: str = \"L\", transform=None, train: bool = True, return_img_ids: bool = False):\n",
    "        \"\"\"\n",
    "            Initialize a torch.utils.data.Dataset object for algonauts2023 dataset\n",
    "\n",
    "            Args:\n",
    "                data_path,              str, path to the algonauts2023 dataset which contains only ONE subject\n",
    "                hemisphere,             str, select which hemisphere of the brain to be modeled\n",
    "                                            can ONLY select \"L\" or \"R\"\n",
    "                                            and ONLY applicable when train is TRUE\n",
    "                transform,              torchvision.transform methods, apply normalization to the dataset\n",
    "                train,                  bool, training data will be loaded if True. Test data otherwise.\n",
    "                return_img_ids,         bool, return image ids, only used for feature extraction\n",
    "        \"\"\"\n",
    "\n",
    "        # collect data paths\n",
    "        path_struct = osp.join(data_path, \"{}_split\")\n",
    "        self.dataset = list()\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.return_img_ids = return_img_ids\n",
    "\n",
    "        if train:\n",
    "            shared_path = osp.join(\n",
    "                path_struct.format(\"training\"), \"training_{}\")\n",
    "            if hemisphere == \"L\":\n",
    "                self.fmri = np.load(osp.join(shared_path.format(\n",
    "                    \"fmri\"), \"lh_training_fmri.npy\"))\n",
    "            elif hemisphere == \"R\":\n",
    "                self.fmri = np.load(osp.join(shared_path.format(\n",
    "                    \"fmri\"), \"rh_training_fmri.npy\"))\n",
    "\n",
    "            self.feature_path = shared_path.format(\"images\")\n",
    "\n",
    "        else:\n",
    "            self.feature_path = osp.join(\n",
    "                path_struct.format(\"test\"), \"test_images\")\n",
    "\n",
    "        self.dataset = list(os.listdir(self.feature_path))\n",
    "\n",
    "        # sorted in ascending order if not train set\n",
    "        if not train:\n",
    "            self.dataset = sorted(self.dataset, key=lambda x: int(\n",
    "                re.findall(\"\\d{4}\", x)[0]) - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "            Load designated sample\n",
    "\n",
    "            Arg:\n",
    "                index,          int, sample id\n",
    "\n",
    "            Returns:\n",
    "                image,          np.ndarray, the 3d numpy array of the image used to retrive fmri data\n",
    "                fmri,           np.ndarray, the hemisphere FMRI data generated by the image\n",
    "                img_ids,        str, image ids, only used for feature extraction\n",
    "        \"\"\"\n",
    "\n",
    "        feat_file = self.dataset[index]\n",
    "        feat_idx = int(re.findall(\"\\d{4}\", feat_file)[0]) - 1\n",
    "        img = Image.open(osp.join(self.feature_path, feat_file))\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(mode=\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.return_img_ids:\n",
    "            return img, self.fmri[feat_idx] if self.train else 0, feat_file\n",
    "        else:\n",
    "            return img, self.fmri[feat_idx] if self.train else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = \"subj08\"\n",
    "train = True\n",
    "tpe = \"training\" if train else \"test\"\n",
    "path = \"/mnt/data/{}\".format(subj)\n",
    "save = \"/mnt/data/{}/{}_split/{}_features/vit-gpt2-image-captioning/decoder-raw\".format(\n",
    "    subj, tpe, tpe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Algonauts2023Raw(path, train=train, return_img_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138/138 [18:01<00:00,  7.84s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def func(x):\n",
    "    \n",
    "    imgs = list()\n",
    "    ids = list()\n",
    "    for img, _, id in x:\n",
    "        imgs.append(img)\n",
    "        ids.append(id)\n",
    "        \n",
    "    return imgs, ids\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "ids = list()\n",
    "features = list()\n",
    "for img, id in tqdm(DataLoader(dset, batch_size=64, num_workers=12, collate_fn=func)):\n",
    "    \n",
    "    pixel_values = feature_extractor(\n",
    "        images=img, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    feats = model.generate(pixel_values, **gen_kwargs)\n",
    "    feats = [x for x in feats.encoder_hidden_states]\n",
    "    feats = torch.stack(feats[-4:]).cpu()\n",
    "    \n",
    "    features.append(feats)\n",
    "    ids.append(id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [06:08,  2.90s/it]"
     ]
    }
   ],
   "source": [
    "for feats, id in tqdm(zip(features, ids)):\n",
    "\n",
    "    for i in range(len(id)):\n",
    "        hs = feats[:,i]\n",
    "        hs = hs.numpy().astype(np.float32)\n",
    "\n",
    "        if not os.path.isdir(os.path.join(save)):\n",
    "            os.makedirs(save)\n",
    "\n",
    "        np.save(os.path.join(save, id[i].split(\".\")[0]+\".npy\"), hs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algonauts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
