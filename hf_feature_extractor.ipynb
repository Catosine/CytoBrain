{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    \"./backbone.nosync/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\n",
    "    \"./backbone.nosync/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./backbone.nosync/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams, \"return_dict_in_generate\": True, \"output_hidden_states\": True}\n",
    "\n",
    "# ['a woman in a hospital bed with a woman in a hospital bed']\n",
    "# predict_step(['doctor.e16ba4e4.jpg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step(image_paths):\n",
    "  images = []\n",
    "  for image_path in image_paths:\n",
    "    i_image = Image.open(image_path)\n",
    "    if i_image.mode != \"RGB\":\n",
    "      i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    images.append(i_image)\n",
    "\n",
    "  pixel_values = feature_extractor(\n",
    "      images=images, return_tensors=\"pt\")\n",
    "\n",
    "  pixel_values = pixel_values.pixel_values\n",
    "  \n",
    "  pixel_values = pixel_values.to(device)\n",
    "  \n",
    "  #outputs = model(pixel_values, decoder_input_embeds=pixel_values)\n",
    "  outputs = model.generate(pixel_values, **gen_kwargs)\n",
    "  print(len(outputs.encoder_hidden_states))\n",
    "  output_ids = outputs.sequences\n",
    "\n",
    "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  return preds\n",
    "\n",
    "\n",
    "predict_step(['../../data.nosync/subj01/test_split/test_images/test-0011_nsd-04735.png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Algonauts2023Raw(Dataset):\n",
    "    \"\"\"\n",
    "        Load original data for Algonauts2023 dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, hemisphere: str = \"L\", transform=None, train: bool = True, return_img_ids: bool = False):\n",
    "        \"\"\"\n",
    "            Initialize a torch.utils.data.Dataset object for algonauts2023 dataset\n",
    "\n",
    "            Args:\n",
    "                data_path,              str, path to the algonauts2023 dataset which contains only ONE subject\n",
    "                hemisphere,             str, select which hemisphere of the brain to be modeled\n",
    "                                            can ONLY select \"L\" or \"R\"\n",
    "                                            and ONLY applicable when train is TRUE\n",
    "                transform,              torchvision.transform methods, apply normalization to the dataset\n",
    "                train,                  bool, training data will be loaded if True. Test data otherwise.\n",
    "                return_img_ids,         bool, return image ids, only used for feature extraction\n",
    "        \"\"\"\n",
    "\n",
    "        # collect data paths\n",
    "        path_struct = osp.join(data_path, \"{}_split\")\n",
    "        self.dataset = list()\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.return_img_ids = return_img_ids\n",
    "\n",
    "        if train:\n",
    "            shared_path = osp.join(\n",
    "                path_struct.format(\"training\"), \"training_{}\")\n",
    "            if hemisphere == \"L\":\n",
    "                self.fmri = np.load(osp.join(shared_path.format(\n",
    "                    \"fmri\"), \"lh_training_fmri.npy\"))\n",
    "            elif hemisphere == \"R\":\n",
    "                self.fmri = np.load(osp.join(shared_path.format(\n",
    "                    \"fmri\"), \"rh_training_fmri.npy\"))\n",
    "\n",
    "            self.feature_path = shared_path.format(\"images\")\n",
    "\n",
    "        else:\n",
    "            self.feature_path = osp.join(\n",
    "                path_struct.format(\"test\"), \"test_images\")\n",
    "\n",
    "        self.dataset = list(os.listdir(self.feature_path))\n",
    "\n",
    "        # sorted in ascending order if not train set\n",
    "        if not train:\n",
    "            self.dataset = sorted(self.dataset, key=lambda x: int(\n",
    "                re.findall(\"\\d{4}\", x)[0]) - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "            Load designated sample\n",
    "\n",
    "            Arg:\n",
    "                index,          int, sample id\n",
    "\n",
    "            Returns:\n",
    "                image,          np.ndarray, the 3d numpy array of the image used to retrive fmri data\n",
    "                fmri,           np.ndarray, the hemisphere FMRI data generated by the image\n",
    "                img_ids,        str, image ids, only used for feature extraction\n",
    "        \"\"\"\n",
    "\n",
    "        feat_file = self.dataset[index]\n",
    "        feat_idx = int(re.findall(\"\\d{4}\", feat_file)[0]) - 1\n",
    "\n",
    "        img = cv2.imread(osp.join(self.feature_path, feat_file)\n",
    "                         ).astype(np.float32)\n",
    "\n",
    "        # convert BGR to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.return_img_ids:\n",
    "            return img, self.fmri[feat_idx] if self.train else 0, feat_file\n",
    "        else:\n",
    "            return img, self.fmri[feat_idx] if self.train else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = \"subj01\"\n",
    "\n",
    "path = \"../../data.nosync/{}\".format(subj)\n",
    "save = \"../../data.nosync/{}/training_split/training_features/vit-gpt2-image-captioning/decoder-raw\".format(\n",
    "    subj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Algonauts2023Raw(path, return_img_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for img, _, id in tqdm(DataLoader(dset, batch_size=16)):\n",
    "\n",
    "    pixel_values = feature_extractor(\n",
    "        images=img, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    feats = model.generate(pixel_values, **gen_kwargs)\n",
    "    feats = feats.encoder_hidden_states\n",
    "\n",
    "    for i in range(len(id)):\n",
    "        hs = [f[i] for f in feats]\n",
    "        hs = torch.stack(hs).cpu().numpy().astype(np.float32)\n",
    "\n",
    "        if not os.path.isdir(os.path.join(save)):\n",
    "            os.makedirs(save)\n",
    "\n",
    "        np.save(os.path.join(save, id[i].split(\".\")[0]+\".npy\"), hs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "avg_save = \"../../data.nosync/{}/training_split/training_features/vit-gpt2-image-captioning/encoder-last-768\".format(\n",
    "    subj)\n",
    "\n",
    "if not os.path.isdir(avg_save):\n",
    "\n",
    "    os.makedirs(avg_save)\n",
    "\n",
    "for x in tqdm(os.listdir(save)):\n",
    "    dat = np.load(os.path.join(save, x))\n",
    "    dat = dat[:, 0].reshape(-1)\n",
    "    \n",
    "    #dat = np.mean(dat, axis=1).reshape(-1)\n",
    "    np.save(os.path.join(avg_save, x), dat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "feats = list()\n",
    "files = os.listdir(save)\n",
    "for x in files:\n",
    "    feats.append(np.load(os.path.join(save, x)).reshape(-1))\n",
    "\n",
    "feats = np.vstack(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=64)\n",
    "\n",
    "pca.fit(X=feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_feats = pca.transform(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_save = \"../../data.nosync/{}/training_split/training_features/vit-gpt2-image-captioning/encoder-pca-64\".format(\n",
    "    subj)\n",
    "\n",
    "if not os.path.isdir(pca_save):\n",
    "\n",
    "    os.makedirs(pca_save)\n",
    "\n",
    "for f, x in zip(reduced_feats, files):\n",
    "    np.save(os.path.join(pca_save, x), f.astype(np.float32))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algonauts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
